{
  "hash": "9c6c313297defd25d366b7b4bf1028df",
  "result": {
    "markdown": "---\ntitle: \"üìù Diagnosing Collinearity\"\nformat:\n  html:\n    code-copy: true\n    code-fold: false\n    highlight-style: zenburn\n    df-print: paged\n    css: [\"../assets/style.css\", \"../assets/notes.css\", \"../assets/table-styles.css\"]\ndate: 09-29-2022\nbibliography: '../assets/epsy8264.bib'\ncsl: '../assets/apa-single-spaced.csl'\n---\n\n::: {.cell}\n\n:::\n\n\n---\n\nIn this set of notes, we will give a brief introduction to empirical diagnostics to detect collinearity. We will use the *equal-education-opportunity.csv* data provided from @Chatterjee:2012 to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n\n- [[CSV]](https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/data/equal-education-opportunity.csv)\n- [[Codebok]](../codebooks/equal-education-opportunity)\n\nA script file for the analyses in these notes is also available:\n\n- [[R Script File]](https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/scripts/06-01-diagnosing-collinearity.R)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Read in data\neeo = read_csv(\"https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/data/equal-education-opportunity.csv\")\nhead(eeo)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"achievement\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"faculty\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"peer\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"school\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-0.43148\",\"2\":\"0.60814\",\"3\":\"0.03509\",\"4\":\"0.16607\"},{\"1\":\"0.79969\",\"2\":\"0.79369\",\"3\":\"0.47924\",\"4\":\"0.53356\"},{\"1\":\"-0.92467\",\"2\":\"-0.82630\",\"3\":\"-0.61951\",\"4\":\"-0.78635\"},{\"1\":\"-2.19081\",\"2\":\"-1.25310\",\"3\":\"-1.21675\",\"4\":\"-1.04076\"},{\"1\":\"-2.84818\",\"2\":\"0.17399\",\"3\":\"-0.18517\",\"4\":\"0.14229\"},{\"1\":\"-0.66233\",\"2\":\"0.20246\",\"3\":\"0.12764\",\"4\":\"0.27311\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Source the residual_plots() function from the script file\nsource(\"../scripts/residual_plots.R\")\n```\n:::\n\n\n<br />\n\n\n# Regression Analysis\n\nTo examine the RQ, the following model was posited:\n\n$$\n\\mathrm{Achievement}_i = \\beta_0 + \\beta_1(\\mathrm{Faculty}_i) + \\beta_2(\\mathrm{Peer}_i) + \\beta_3(\\mathrm{School}_i) + \\epsilon_i\n$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Residual plots for the model that includes the main effects of faculty credentials, influence of peer groups, and measure of school facilities to predict variation in student achievement.](06-01-diagnosing-collinearity_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=60%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fit the regression model\n# Index plots of several regression diagnostics\ninfluenceIndexPlot(lm.1)\n```\n\n::: {.cell-output-display}\n![Diagnostic plots for the model that includes the main effects of faculty credentials, influence of peer groups, and measure of school facilities to predict variation in student achievement.](06-01-diagnosing-collinearity_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nSchool 28 may be problematic, but removing this observation (work not shown) made little improvement in the residual plots. As such, School 28 was retained in the data. As the assumptions seem reasonably met, we next look to the model-level and coefficient-level output:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Model-level information\nprint(glance(lm.1), width = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.206         0.170  2.07      5.72 0.00153     3  -148.  306.  318.\n  deviance df.residual  nobs\n     <dbl>       <int> <int>\n1     283.          66    70\n```\n:::\n\n```{.r .cell-code}\n# Coefficient-level information\ntidy(lm.1, conf.int = 0.95)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"-0.06995905\",\"3\":\"0.2506421\",\"4\":\"-0.2791193\",\"5\":\"0.7810260\",\"6\":\"-0.5703821\",\"7\":\"0.430464\"},{\"1\":\"faculty\",\"2\":\"1.10125968\",\"3\":\"1.4105616\",\"4\":\"0.7807243\",\"5\":\"0.4377560\",\"6\":\"-1.7150174\",\"7\":\"3.917537\"},{\"1\":\"peer\",\"2\":\"2.32205659\",\"3\":\"1.4812872\",\"4\":\"1.5675937\",\"5\":\"0.1217584\",\"6\":\"-0.6354288\",\"7\":\"5.279542\"},{\"1\":\"school\",\"2\":\"-2.28099511\",\"3\":\"2.2204481\",\"4\":\"-1.0272679\",\"5\":\"0.3080446\",\"6\":\"-6.7142627\",\"7\":\"2.152272\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nExamining this information we find:\n\n- 20% of the variation in student achievement is explained by the model; which is statistically significant $F(3, 66)=5.72$; $p=0.002$.\n- However, none of the individual coefficients are statistically significant!\n\nThis is a bit of a paradox since we rejected the model-level null hypothesis that $H_0:\\beta_{\\mathrm{Faculty~Credentials}}=\\beta_{\\mathrm{Peer~Influence}}=\\beta_{\\mathrm{School~Facilities}}=0$, yet the coefficient-level results are consistent with $H_0:\\beta_{\\mathrm{Faculty~Credentials}}=0$, $H_0:\\beta_{\\mathrm{Peer~Influence}}=0$, and $H_0:\\beta_{\\mathrm{School~Facilities}}=0$. These inconsistencies between the model- and coefficient-level results are typical when there is *collinearity* in the model.\n\n<br />\n\n\n# What is Collinearity?\n\nRecall from our introduction to matrix algebra that two vectors are collinear if they span the same subspace. In regression, collinearity occurs when any of the columns of the design matrix, **X**, is a perfect linear combination of the other columns:\n\n$$\n\\mathbf{X_j} = c_0(\\mathbf{1}) + c_1\\mathbf{X_1} + c_2\\mathbf{X_2} + c_3\\mathbf{X_3} + \\ldots + c_k\\mathbf{X_k}\n$$\n\nand the constants, $c_1, c_2, c_3,\\ldots, c_k$ are not all 0. In this situation, **X** is not of full column rank, and the $\\mathbf{X}^{\\intercal}\\mathbf{X}$ matrix is singular. \n\n<aside>\nSometimes 'collinearity' is referred to as 'multicollinearity'. The two terms are synonomous.\n</aside>\n\n\n<br />\n\n\n## Effects of Collinearity\n\nIf the design matrix is not of full rank, and $\\mathbf{X}^{\\intercal}\\mathbf{X}$ is singular, then the OLS normal equations do not have a unique solution. Moreover, the sampling variances for the coefficient are all infinitely large. To understand why this is the case, we can examine one formula for the sampling variance of a slope in a multiple regression:\n\n\n$$\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n$$\n\nwhere \n\n- $R^2_j$ is the squared multiple correlation for the regression of $X_j$ on the the other predictors;\n- $S^2_j$ is the sample variance of predictor $X_j$ defined by $S^2_j = \\dfrac{\\sum(X_{ij}-\\bar{X}_j)^2}{n-1}$;\n- $\\sigma^2_{\\epsilon}$ is the variance of the residuals based on regressing $Y$ on all the $X$'s \n\n<aside>\nRecall that the multiple correlation is the correlation between the outcome and the predicted values.\n</aside>\n\nThe first term in this product is referred to as the *variance inflation factor* (VIF). When one of the predictors is perfectly collinear with the others, the value of $R^2_j$ is 1 and the VIF is infinity. Thus the sampling variance of $B_j=\\infty$.\n\n<br />\n\n\n## Perfect Collinearity in Practice: Model Mis-specification\n\nIn practice, it is unlikely that you will have exact (or perfect) collinearity. When it does happen it is often the result of mis-formulating the model (e.g., including dummy variables in the model for all levels of a categorical variable, as well as the intercept). As an example of this, imagine that you were creating the design matrix for a regression model that included occupational status (employed/not employed) to predict some outcome for 5 cases.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create design matrix\nX = data.frame(\n  b_0 = rep(1, 5),\n  employed = c(1, 1, 0, 0, 1),\n  not_employed = c(0, 0, 1, 1, 0)\n)\n\n# View design matrix\nX\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"b_0\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"employed\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"not_employed\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"0\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"0\"},{\"1\":\"1\",\"2\":\"0\",\"3\":\"1\"},{\"1\":\"1\",\"2\":\"0\",\"3\":\"1\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nThe columns in this design matrix are collinear because we can express any one of the columns as a linear combination of the others. For example,\n\n$$\nb_0 = 1(\\mathrm{employed}) + 1(\\mathrm{not~employed})\n$$\n\nChecking the rank of this matrix, we find that this matrix has a rank of 2. Since there are three columns, **X** is not full column rank; it is rank deficient.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMatrix::rankMatrix(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\nattr(,\"method\")\n[1] \"tolNorm2\"\nattr(,\"useGrad\")\n[1] FALSE\nattr(,\"tol\")\n[1] 1.110223e-15\n```\n:::\n:::\n\n\nIncluding all three coefficients in the model results in overparameterization. The simple solution here is to drop one of the predictors from the model. This is why we only include a single dummy variable in a model that includes an intercept for a dichotomous categorical predictor.\n\n<aside>\nIncluding the intercept is not imperative, although it has a useful interpretation when using dummy coding. One could also include the two dummy-coded predictors and omit the intercept. This gives the means for the two groups, but does not provide a comparison of those means.\n</aside>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create vector of outcomes\nY = c(15, 15, 10, 15, 30)\n\n# Create data frame of Y and X\nmy_data = cbind(Y, X)\nmy_data\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Y\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"b_0\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"employed\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"not_employed\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"15\",\"2\":\"1\",\"3\":\"1\",\"4\":\"0\"},{\"1\":\"15\",\"2\":\"1\",\"3\":\"1\",\"4\":\"0\"},{\"1\":\"10\",\"2\":\"1\",\"3\":\"0\",\"4\":\"1\"},{\"1\":\"15\",\"2\":\"1\",\"3\":\"0\",\"4\":\"1\"},{\"1\":\"30\",\"2\":\"1\",\"3\":\"1\",\"4\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Coefficients (including all three terms)\ncoef(lm(Y ~ 1 + employed + not_employed, data = my_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept)     employed not_employed \n        12.5          7.5           NA \n```\n:::\n\n```{.r .cell-code}\n# Coefficients (omitting intercept)\ncoef(lm(Y ~ -1 + employed + not_employed, data = my_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    employed not_employed \n        20.0         12.5 \n```\n:::\n:::\n\n\nIf you overparameterize a model with `lm()`, one or more of the coefficients will not be estimated (the last parameters entered in the model).\n\n<aside>\nConstraining some parameters is another way to produce a full rank design matrix. For example the ANOVA model has a constraint that the sum of the effect-coded variable is 0. This constraint ensures that the design matrix will be of full rank.\n</aside>\n\n<br />\n\n\n## Non-Exact Collinearity\n\nIt is more likely, in practice, that you will have less-than-perfect collinearity, and that this will have an adverse effect on the computational estimates of the coefficients' sampling variances. Again, we look toward how the sampling variances for the coefficent's are computed:\n\n\n$$\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n$$\n\n\nWhen the predictors are completely independent, all of the columns of the design matrix will be orthogonal and the correlation between $X_j$ and the other $X$s will be 0. In this situation, the VIF is 1 and the second term in the product completely defines the sampling variance. This means that the sampling variance is a function of the model's residual variance, sample size, and the predictor's variance---the factors we typically think of affecting the sampling variance of a coefficient.\n\n\nIn cases where the columns in ths design matrix are not perfectly orthogonal, the correlation between $X_j$ and the other $X$s is larger than 0. (Perfect collinearity results in $R^2_j=1$.) For these situations, the VIF has a value that is greater than 1. When this happens the VIF acts as a multiplier of the second term, inflating the the sampling variance and reducing the precision of the estimate (i.e., increasing the uncertainty).\n\nHow much the uncertainty in the estimate increases is a function of how correlated the predictors are. Here we can look at various multiple correlations ($R_j$) between $X_j$ and the predicted values from using the other $X$'s to predict $X_j$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table style='width:100%;'>\n<caption>Impact of various R<sub>j</sub> values on the VIF and size of the CI for B<sub>j</sub>.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> R<sub>j</sub> </th>\n   <th style=\"text-align:right;\"> VIF </th>\n   <th style=\"text-align:right;\"> CI Factor </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 1.00 </td>\n   <td style=\"text-align:right;\"> 1.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.1 </td>\n   <td style=\"text-align:right;\"> 1.01 </td>\n   <td style=\"text-align:right;\"> 1.01 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 1.04 </td>\n   <td style=\"text-align:right;\"> 1.02 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.3 </td>\n   <td style=\"text-align:right;\"> 1.10 </td>\n   <td style=\"text-align:right;\"> 1.05 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.4 </td>\n   <td style=\"text-align:right;\"> 1.19 </td>\n   <td style=\"text-align:right;\"> 1.09 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.5 </td>\n   <td style=\"text-align:right;\"> 1.33 </td>\n   <td style=\"text-align:right;\"> 1.15 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.6 </td>\n   <td style=\"text-align:right;\"> 1.56 </td>\n   <td style=\"text-align:right;\"> 1.25 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.7 </td>\n   <td style=\"text-align:right;\"> 1.96 </td>\n   <td style=\"text-align:right;\"> 1.40 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.8 </td>\n   <td style=\"text-align:right;\"> 2.78 </td>\n   <td style=\"text-align:right;\"> 1.67 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.9 </td>\n   <td style=\"text-align:right;\"> 5.26 </td>\n   <td style=\"text-align:right;\"> 2.29 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1.0 </td>\n   <td style=\"text-align:right;\"> Inf </td>\n   <td style=\"text-align:right;\"> Inf </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nFor example, a multiple correlation of 0.7 results in a VIF of 1.96, which in turn means that the CI (which is based on the square root of the sampling variance) will increase by a factor of 1.4. This inflation increases the uncertainty of the estimate making it harder to make decisions or understand the effect of $B_j$.\n\nTo sum things up, while perfect collinearity is rare in practice, less-than-perfect collinearity is common. In these cases the VIF will be less than 1, but can still have an adverse effect on the sampling variances; sometimes making them quite large.\n\n<br />\n\n\n# Identifying Collinearity\n\nIn our case study example, we were alerted to the possible collinearity by finding that the predictors jointly were statistically significant, but that each of the individual predictors were not. Other signs that you may have collinearity problems are:\n\n- Large changes in the size of the estimated coefficients when variables are added to the model;\n- Large changes in the size of the estimated coefficients when an observation is added or deleted;\n- The signs of the estimated coefficients do not conform to their prior substantively hypothesized directions;\n- Large SEs on variables that are expected to be important predictors.\n\n<br />\n\n\n## Collinearity Diagnostics\n\nWe can also empirically diagnose problematic collinearity in the data [@Belsley:1991; @Belsley:1980]. Before we do, however, it is important that the **functional form of the model** has been correctly specified. Since, a model needs to be specified before we can estimate coefficients or their sampling variances, and collinearity produces unstable estimates of these estimates, collinearity should only be investigated *after* the model has been satisfactorily specified.\n\nBelow we will explore some of the diagnostic tools available to an applied researcher.\n\n<br />\n\n\n## High Correlations among Predictors\n\nCollinearity can sometimes be anticipated by examining the pairwise correlations between the predictors. If the correlation between predictors is large, this might be indicative of collinearity problems.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neeo |>\n  select(faculty, peer, school) |>\n  correlate()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"faculty\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"peer\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"school\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"faculty\",\"2\":\"NA\",\"3\":\"0.9600806\",\"4\":\"0.9856837\"},{\"1\":\"peer\",\"2\":\"0.9600806\",\"3\":\"NA\",\"4\":\"0.9821601\"},{\"1\":\"school\",\"2\":\"0.9856837\",\"3\":\"0.9821601\",\"4\":\"NA\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nIn this example, all three of the predictors are highly correlated with one another. This is likely a good indicator that their may be problems in the estimation of coefficients, inflated standard errors, or both; especially given that the correlations are all very high. Unfortunately the source of collinearity may be due to more than just the simple relationships among the predictors. As such, just examining the pairwise correlations is not enough to detect collinearity (although it is a good first step).\n\n<br />\n\n\n## Regress each Predictor on the Other Predictors\n\nSince collinearity is defined as linear dependence within the set of predictors, a better way to diagnose collinearity than just examining the pairwise correlation coefficients is to regress each of the predictors on the remaining predictors and evaluate the $R^2$ value. If all the $R^2$ values are close to zero there is no collinearity problems. If one or more of the $R^2$ values are close to 1, there is a collinearity problem.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Use faculty as outcome; obtain R2\nsummary(lm(faculty ~ 1 + peer + school, data = eeo))$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9733906\n```\n:::\n\n```{.r .cell-code}\n# Use faculty as outcome; obtain R2\nsummary(lm(peer ~ 1 + faculty + school, data = eeo))$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9669002\n```\n:::\n\n```{.r .cell-code}\n# Use faculty as outcome; obtain R2\nsummary(lm(school ~ 1 + faculty + peer, data = eeo))$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9879743\n```\n:::\n:::\n\n\nAll three $R^2$ values are quite high, which is indicative of collinearity.\n\nOne shortcoming with this method of diagnosing collinearity is that when the predictor space is large, you would need to look at the $R^2$ values from several models. And, while this could be automated in an R function, there are other common methods that allow us to diagnose collinearity.\n\nWe will examine three additional common methods statisticians use to empirically detect collinearity: (1) computing variance inflation factors for the coefficients; (2) examining the eigenvalues of the correlation matrix; and (3) examining the condition indices of the correlation matrix.\n\n<br />\n\n\n## Variance Inflation Factor (VIF)\n\nPerhaps the most common method applied statisticians use to diagnose collinaerity is to compute and examine variance inflation factors. Recall that the variance inflation factor (VIF) is an indicator of the degree of collinearity, where VIF is:\n\n$$\n\\mathrm{VIF} = \\frac{1}{1 - R^2_j}\n$$\n\nThe VIF impacts the size of the variance estimates for the regression coefficients, and as such, can be used as a diagnostic of collinearity. In practice, since it is more conventional to use the SE to measure uncertainty, it is typical to use the square root of the VIF as a diagnostic of collinearity in practice. The square root of the VIF expresses the proportional change in the CI for the coefficients. We can use the `vif()` function from the **car** package to compute the variance inflation factors for each coefficient.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# VIF\nvif(lm.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n faculty     peer   school \n37.58064 30.21166 83.15544 \n```\n:::\n\n```{.r .cell-code}\n# Square root of VIF\nsqrt(vif(lm.1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n faculty     peer   school \n6.130305 5.496513 9.118960 \n```\n:::\n:::\n\n\nThe variances (and hence, the standard errors) for all three coefficients are inflated because of collinearity. The SEs for these coefficients are all more than five times as large as they would be if the predictors were independent.\n\nRemember, the VIF can range from 1 (independence among the predictors) to infinity (perfect collinearity). There is not consensus among statisticians about how high the VIF has to be to constitute a problem. Some references cite $\\mathrm{VIF}>10$ as problematic (which increases the size of the CI for the coefficient by a factor of over three); while others cite $\\mathrm{VIF}>4$ as problematic (which increases the size of the CI for the coefficient by a factor of two). As you consider what VIF value to use as an indicator of problematic inflation, it is more important to consider what introducing that much uncertainty would mean in your substantive problem. For example, would you be comfortable with tripling the uncertainty associated with the coefficient? What about doubling it? Once you make that decision, you can determine your VIF cutoff.\n\nThere are several situations in which high VIF values are expected and not problematic:\n\n- **The variables with high VIFs are control variables, and the variables of interest do not have high VIFs.** Since we would not be interested in inference around the control variables, high VIF values on those variables would not\n- **The high VIFs are caused by the inclusion of powers or products of other variables.** The *p*-value for a product term is not affected by the multicollinearity. Centering predictors prior to creating the powers or the products will reduce the correlations, but the *p*-value the products will be exactly the same whether or not you center. Moreover the results for the other effects will be the same in either case indicating that multicollinearity has no adverse consequences.\n- **The variables with high VIFs are indicator (dummy) variables that represent a categorical variable with three or more categories.** This is especially true when the reference category used has a small proportion of cases. In this case, *p*-values for the indicator variables may be high, but the overall test that all indicators have coefficients of zero is unaffected by the high VIFs. And nothing else in the regression is affected. To avoid the high VIF values in this situaton, just choose a reference category with a larger proportion of cases.\n\n<br />\n\n\n## Eigenvalues of the Correlation Matrix\n\nA second common method of evaluating collinearity is to compute and evaluate the eigenvalues of the correlation matrix for the predictors. Recall that each square ($k \\times k$) matrix has a set of *k* scalars, called eigenvalues (denoted $\\lambda$) associated with it. These eigenvalues can be arranged in descending order such that,\n\n$$\n\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq \\ldots \\geq \\lambda_k\n$$\n\nBecause any correlation matrix is a square matrix, we can find a corresponding set of eigenvalues for the correlation matrix. If any of these eigenvalues is exactly equal to zero, it indicates a linear dependence among the variables making up the correlation matrix. \n\nAs a diagnostic, rather than looking at the size of all the eigenvalues, we compute the sum of the reciprocals of the eigenvalues:\n\n$$\n\\sum_{i=1}^k \\frac{1}{\\lambda_i}\n$$\n\n- If the predictors are orthogonal to one another (independent) then $\\lambda_i = 1$ and the sum of the reciprocal values will be equal to the number of predictors, $\\sum_{i=1}^k \\frac{1}{\\lambda_i} = k$.\n- If the predictors are collinear with one another (dependent) then $\\lambda_i = 0$ and the sum of the reciprocal values will be equal to infinity, $\\sum_{i=1}^k \\frac{1}{\\lambda_i} = \\infty$.\n- When there is nonperfect collinearity then $0 < \\lambda_i < 1$, and the sum of the reciprocal values will be greater than the number of predictors, $\\sum_{i=1}^k \\frac{1}{\\lambda_i} > k$.\n\n\n:::math\nIn an orthogonal matrix, the eigenvalues are all $\\pm1$, but since the correlation matrix is positive semidefinite, the eigenvalues are all $+1$.\n:::\n\nLarger sums of the reciprocal values of the eigenvalues is indicative of higher degrees of collinearity. In practice, we might use some cutoff to indicate when the collinearity is problematic. One such cutoff used is, if the sum is greater than five times the number of predictors, it is a sign of collinearity.\n\n$$\n\\mathrm{IF} \\quad \\sum_{i=1}^k \\frac{1}{\\lambda_i} > 5k \\quad \\mathrm{THEN} \\quad \\mathrm{collnearity~is~a~problem}\n$$\n\n<br />\n\n\n\n:::fyi\nIn practice, perfect collinearity is rare, but near perfect collinearity can exist and is indicated when at least one of the eigenvalues is near zero, and is quite a bit smaller than the others.\n:::\n\n\n<br />\n\n\n### Using R to Compute the Eigenvalues of the Correlation Matrix\n\nBecause collinearity indicates dependence among the predictors, we would want to compute the eigenvalues for the correlation matrix of the predictors (do not include the outcome when computing this matrix). We can then use the `eigen()` function to compute the eigenvalues of a square matrix. \n\nIn previous classes, I have been using the `correlate()` function from the `{corrr}` package to produce correlation matrices. This function produces a formatted output that is nice for displaying the correlation matrix, but, because of its formatting, is not truly a matrix object. Instead, we will use the `cor()` function, which produces a matrix object, to produce the correlation matrix.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Correlation matrix of predictors\nr_xx = cor(eeo[c(\"faculty\", \"peer\", \"school\")])\nr_xx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          faculty      peer    school\nfaculty 1.0000000 0.9600806 0.9856837\npeer    0.9600806 1.0000000 0.9821601\nschool  0.9856837 0.9821601 1.0000000\n```\n:::\n:::\n\n\nOnce we have the correlation matrix, we can use the `eigen()` function to compute the eigenvalues (and eigenvectors) of the inputted correlation matrix.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compute eigenvalues and eigenvectors\neigen(r_xx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1] 2.951993158 0.040047507 0.007959335\n\n$vectors\n           [,1]        [,2]       [,3]\n[1,] -0.5761385  0.67939712 -0.4544052\n[2,] -0.5754361 -0.73197527 -0.3648089\n[3,] -0.5804634  0.05130072  0.8126687\n```\n:::\n\n```{.r .cell-code}\n# Sum of reciprocal of eigenvalues\nsum(1 / eigen(r_xx)$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 150.9477\n```\n:::\n:::\n\n\nWe compare the sum of the reciprocal of the eigenvalues to five times the number of predictors; $5 \\times 3 =15$. Since this sum is greater than 15, we would conclude that there is a collinearity problem for this model.\n\n\n<br />\n\n\n## Condition Indices\n\nA condition number for a matrix and computational task quantifies how sensitive the result is to perturbations in the input data and to roundoff errors made during the solution process. If minor changes to the matrix elements result in large differences in the computation (say of the inverse), we say that the matrix is \"ill-conditioned\". It is important to note that a condition number applies not only to a particular matrix, but also to the particular computation being carried out. That is, a matrix can be ill-conditioned for inversion while the eigenvalue problem is well-conditioned.\n\nA third common diagnostic measure of collinearity is to compute the *condition number* of the correlation matrix of the model predictors. This tells us whether small changes in the data will lead to large changes in regression coefficient estimates. To compute the condition number, we need to compute the *condition index* for each of the eigenvalues of the correlation matrix based on the model predictors. Each eigenvalue has an associated condition index, and the *j*th eigenvalue's condition index is denoted $\\kappa_j$, where,\n\n$$\n\\kappa_j = \\sqrt{\\frac{\\lambda_{\\mathrm{Max}}}{\\lambda_j}}\n$$\n\nand $\\lambda_{\\mathrm{Max}}$ is the largest eigenvalue. The largest eigenvalue will have a condition index of,\n\n$$\n\\begin{split}\n\\kappa_{\\lambda_\\mathrm{Max}} &= \\sqrt{\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Max}}} \\\\[1ex]\n&= 1\n\\end{split}\n$$\n\n\nThe smallest eigenvalue will have a condition index of\n\n$$\n\\kappa_{\\lambda_\\mathrm{Min}} = \\sqrt{\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Min}}} \n$$\n\nThis value will be larger than 1 since $\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Min}}$ will be greater than 1. In general, the largest eigenvalue will have a condition index of 1 and the other condition indices for every other eigenvalue will be larger than one.\n\n\nThe condition number of the correlation matrix is equivalent to the condition index for the smallest eigenvalue, that is,\n\n\n$$\n\\kappa = \\sqrt{\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Min}}}\n$$\n\nIf the condition number is small, it indicates that the predictors are not collinear, whereas large condition numbers are evidence supporting collinearity.\n\nFrom empirical work, a condition number between 10 and 30 indicates the presence of multicollinearity. When the condition number is larger than 30, the multicollinearity is regarded as strong and corrective action will almost surely need to be taken. Below we compute the condition indices and the condition number for our empirical example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Sort eigenvalues from largest to smallest\nlambda = sort(eigen(r_xx)$values, decreasing = TRUE)\n\n# View eigenvalues\nlambda\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.951993158 0.040047507 0.007959335\n```\n:::\n\n```{.r .cell-code}\n# Compute condition indices\nsqrt(max(lambda) / lambda)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  1.000000  8.585586 19.258359\n```\n:::\n\n```{.r .cell-code}\n# Compute condition number directly\nsqrt(max(lambda) / min(lambda))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 19.25836\n```\n:::\n:::\n\n\nThe condition number of the correlation matrix, $\\kappa = 19.26$, suggests there is collinearity among the predictors. \n\n\n<br />\n\n\n# Fixing Collinearity in Practice\n\nThe presence of collinearity in the predictor space leads to several potential problems, namely that the estimates of the regression coefficients may not be stable (small changes in the data may lead to big changes in the estimates), and that the standard errors/sampling variances are inflated.\n\nAlthough there are several solutions to \"fix\" collinearity in practice, none are a magic bullet. Here are three potential fixes:\n\n- Re-specify the model\n    - Drop one (or more) of the collinear predictors---This changes what you are controlling for;\n    - Combine collinear predictors;\n- Biased estimation\n    - Trade small amount of bias for a reduction in coefficient variability;\n- Introduce prior information about the coefficients\n    - This can be done formally in the analysis (e.g., Bayesian analysis);\n    - It can be used to give a different model specification.\n\nNote that although collinearity is a data problem, the most common fixes in practice are to change the model. In upcoming notes, we will look at methods for combining collinear predictors and performing biased estimation.\n\nFor example, we could alleviate the collinearity by dropping any two of the predictors and re-fitting the model with only one predictor. This would fix the problem, but would be unsatisfactory because the resulting model would not allow us to answer the research question.\n\nThe highly correlated relationships between the predictors is an inherent characteristic of the data generating process we are studying. This makes it difficult to estimate the individual effects of the predictors. Instead, we could look for underlying causes that would explain the relationships we found among the predictors and perhaps re-formulate the model using these underlying causes.\n\n<br />\n\n\n<!-- # References -->\n\n<!-- <br /> -->\n\n",
    "supporting": [
      "06-01-diagnosing-collinearity_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}