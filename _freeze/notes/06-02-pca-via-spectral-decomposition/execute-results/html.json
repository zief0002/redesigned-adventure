{
  "hash": "91d43cbccfeb7a058d07042e70192259",
  "result": {
    "markdown": "---\ntitle: \"üìù Principal Components Analysis via Spectral Decomposition\"\nformat:\n  html:\n    code-copy: true\n    code-fold: false\n    highlight-style: zenburn\n    df-print: paged\n    css: [\"../assets/style.css\", \"../assets/notes.css\", \"../assets/table-styles.css\"]\ndate: 08-05-2023\nbibliography: '../assets/epsy8264.bib'\ncsl: '../assets/apa-single-spaced.csl'\n---\n\n::: {.cell}\n\n:::\n\n\n---\n\nIn this set of notes, we will give a brief introduction to principal components analysis via spectral decomposition. We will continue to use the *equal-education-opportunity.csv* data provided from @Chatterjee:2012 to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n\n- [[CSV]](https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/data/equal-education-opportunity.csv)\n- [[Codebok]](../codebooks/equal-education-opportunity.html)\n  \nA script file for the analyses in these notes is also available:\n\n- [[R Script File]](https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/scripts/06-02-pca-via-spectral-decomposition.R)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Read in data\neeo = read_csv(\"https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/data/equal-education-opportunity.csv\")\nhead(eeo)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"achievement\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"faculty\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"peer\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"school\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-0.43148\",\"2\":\"0.60814\",\"3\":\"0.03509\",\"4\":\"0.16607\"},{\"1\":\"0.79969\",\"2\":\"0.79369\",\"3\":\"0.47924\",\"4\":\"0.53356\"},{\"1\":\"-0.92467\",\"2\":\"-0.82630\",\"3\":\"-0.61951\",\"4\":\"-0.78635\"},{\"1\":\"-2.19081\",\"2\":\"-1.25310\",\"3\":\"-1.21675\",\"4\":\"-1.04076\"},{\"1\":\"-2.84818\",\"2\":\"0.17399\",\"3\":\"-0.18517\",\"4\":\"0.14229\"},{\"1\":\"-0.66233\",\"2\":\"0.20246\",\"3\":\"0.12764\",\"4\":\"0.27311\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Source the residual_plots() function from the script file\nsource(\"../scripts/residual_plots.R\")\n```\n:::\n\n\nThe problem we faced from the last set of notes, was that the predictors in the model were collinear, so we encountered computational issues when trying to estimate the effects and standard errors. One method to deal with collinearity among a set of predictors is to combine the predictors into a smaller subset of orthogonal measures (called principal components) that can be used instead of the original predictors. This subset of measures will not have the collinearity problems (they are orthogonal to one another), but constitute a slightly smaller amount of \"variance accounted for\" than the original set of predictors. \n\n<br />\n\n\n# Idea of Principal Components\n\nIf the *X*-matrix of the predictors were orthogonal, there would be no collinearity issues and we could easily estimate the effects and standard errors. This is, of course, not the case in our example in which the predictors are highly correlated. A scatterplot representing the relationship between two of the three predictors from our EEO data is shown on the left-hand side of the figure below. We can see the highly correlated nature of these variables in the plot. \n\n\nThe idea of principal components analysis is to rotate the basis vectors (coordinate system) so that the axes of the rotated basis correspond to the primary axes of the data ellipse. (Such a rotation of the basis vectors is shown in the center panel of the figure below.) When the coordinates of the predictors are expressed in this new coordinate system, the corresponding 'variables' are orthogonal to one another. The new coordinates of the observations (under the rotated basis vectors) is shown in the right-hand side of the figure below.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![LEFT: Faculty credential and peer influence measures shown in the coordinate system given by the $(1,0)$-$(0,1)$ basis. CENTER: The coordinate system has been rotated based to a new basis. RIGHT: The new coordinates of the observations based on the rotated coordinate space.](06-02-pca-via-spectral-decomposition_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nIf we had considered all three predictors, the plot would be in three-dimensional space and we would need to rotate the coordinate system formed by the basis vectors (1, 0, 0)-(0, 1, 0)-(0, 0, 1). With three dimensions, of course, we can now rotate in multiple directions. This idea can also be extended to *k*-dimensional space. (For now, to keep it simple, we will continue to work with the predictor space defined by the `faculty` and `peer` predictors.)\n\nRecall from the chapter [Basis Vectors and Matrices](https://zief0002.github.io/matrix-algebra/basis-vectors-and-matrices.html), that transforming the coordinates for a point to a new basis is a simple matter of pre-multiplying the vector of original coordinates by the matrix composed of the basis vectors. For example, the first observation in the `eeo` data had a `faculty` value of 0.608, and a `peer` value of 0.0351. If we transform this using a new set of basis vectors, say \n\n$$\n\\begin{bmatrix}0.7625172 & 0.6469680 \\\\ 0.6469680 & -0.7625172\\end{bmatrix}\n$$\n\nThen the same point has the coordinates (0.486, 0.367) under the new basis.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Coordinates in original predictor space (row vector)\nold = t(c(0.608, 0.0351))\n\n# Matrix of new basis vectors\nbasis = matrix(c(0.7625172, 0.6469680, 0.6469680, -0.7625172), nrow = 2)\n\n# Coordinates in rotated predictor space\nold %*% basis \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]      [,2]\n[1,] 0.486319 0.3665922\n```\n:::\n:::\n\n\n<br />\n\n\n# Determining the Principal Components\n\nThe set of rotated basis vectors, referred to as the set of *principal components*, is chosen so that: \n\n(1) the basis vectors are orthogonal to one another, and \n(2) to maximize variance in the predictor space. For example, the direction of the first basis vector (i.e., the first principal component) is chosen to maximize the variation in the predictor space. \n\nMaximizing the variation in the predictor space essentially boils down to rotating the basis so that one of the basis vectors points in the direction of the major axis in the data ellipse. The second principal component is then chosen to maximize variance in an orthogonal direction to the first principal component. (With only two predictors, there is only one possible direction for the second principal component.) In our data ellipse this would be the direction of the minor axis. If the predictor space has more than two dimensions, this process continues until we exhaust the number of principal components.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Data ellipse showing the directions of the principal components. The first principal component is in the direction of the major axis and the second principal component is in the direction of the minor axis.](../img/pca-conceptual.png){fig-align='center' width=80%}\n:::\n:::\n\n\nFinding the rotated set of basis vectors essentially comes down to determining the eigenvalues and eigenvectors for the matrix $\\mathbf{X}_p^\\intercal\\mathbf{X}_p$, where $\\mathbf{X}_p$ is a matrix of the predictors. Since this matrix is square, we can use [spectral decomposition (i.e., eigen decomposition)](https://zief0002.github.io/matrix-algebra/spectral-decompostion.html) to decompose this matrix as follows:\n\n$$\n\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\n$$\n\nwhere the columns of **P** are the eigenvectors of the $\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal$ matrix, and **D** is a diagonal matrix whose diagonal elements are composed of the eigenvalues of the $\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal$ matrix. \n\nThe matrices **P** and $\\mathbf{P}^\\intercal$ are both orthogonal basis matrices that ultimately act to change the coordinate system by rotating the original basis vectors used in the predictor space. The **D** matrix is diagonalizing the $\\mathbf{X}_p^\\intercal\\mathbf{X}_p$ matrix which amounts to finding the the major axes in the data ellipse along which our data varies. \n\n<br />\n\n\n## Matrix Algebra to Carry Out the PCA using Spectral Decomposition\n\n\nTo carry out the spectral decomposition in R, we need to create the matrix of the predictors ($\\mathbf{X}_p$), compute the $\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal$ matrix, and then use the `eigen()` function to carry out the spectral decomposition.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create predictor matrix\nX_p = eeo |>\n  select(peer, faculty) |>\n  data.matrix()\n\n# Spectral decomposition\nspec_decomp = eigen(t(X_p) %*% X_p)\nspec_decomp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1] 137.561000   2.724415\n\n$vectors\n          [,1]       [,2]\n[1,] 0.6469680 -0.7625172\n[2,] 0.7625172  0.6469680\n```\n:::\n:::\n\n\nThe matrix of eigenvectors, in the `$vectors` component, compose the **P** matrix and make up the set of basis vectors for the rotated predictor space. The elements in the `$values` component are the diagonal elements in **D** and are the eigenvalues. The decomposition is:\n\n$$\n\\begin{split}\n\\mathbf{X}^{\\intercal}_p\\mathbf{X}_p &= \\mathbf{PDP}^\\intercal \\\\[1em]\n\\begin{bmatrix}59.16 & 66.52 \\\\ 66.62 & 81.12\\end{bmatrix} &= \\begin{bmatrix}0.647 & -0.763 \\\\ 0.762 & 0.647\\end{bmatrix} \\begin{bmatrix}137.561 & 0  \\\\ 0 & 2.724 \\end{bmatrix} \\begin{bmatrix}0.647 & -0.763 \\\\ 0.762 & 0.647\\end{bmatrix}^\\intercal\n\\end{split}\n$$\n\nThe span of the basis vectors define the principal components, with the first eigenvector defining the first principal component and the second eigenvector defining the second principal component. (Note: There number of principal components will always be the same as the number of predictors in the $\\mathbf{X}_p$ matrix.)\n\nWe can post-multiply the matrix of the original predictor values (in $\\mathbf{X}_p$) by this matrix of basis vectors to obtain the predictor values in the rotated space.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Matrix of basis vectors for rotated predictor space\nrot_basis = spec_decomp$vectors\n\n# Compute rotated values under the new basis\nrot_pred = X_p %*% rot_basis\nhead(rot_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]        [,2]\n[1,]  0.48641930  0.36669037\n[2,]  0.91525519  0.14806327\n[3,] -1.03087107 -0.06220262\n[4,] -1.74270855  0.11707722\n[5,]  0.01287131  0.25376126\n[6,]  0.23695822  0.03365744\n```\n:::\n:::\n\n\nFor example, the first observation, has a value of 0.486 on the first principal component and a value of 0.367 on the second principal component. Similarly, the second observation has a value of 0.915 on the first principal component and a value of 0.148 on the second principal component. Each observation has a set of values on the principal components.\n\nThe eigenvalues are related to the variances of the principal components. Because we decomposed the $\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal$ matrix, the variance on each prinicpal component can be computed as:\n\n$$\n\\mathrm{Var}(\\mathrm{PC}_i) = \\frac{\\lambda_i}{n-1}\n$$\n\nIn our example, the variances can be computed as:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compute variances of PCs\nvar_pc = spec_decomp$values / (70 - 1)\nvar_pc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.99363769 0.03948427\n```\n:::\n:::\n\n\nThe first principal component has the largest variance, which will always be the case. Remember, the principal components are selected so the first component maximizes the variation in the predictor space, the second component will maximize the remaining variance (and be orthogonal to the first), etc.\n\nWe can use these variances to determine the proportion of variation in the predictor space that each principal component accounts for. This is often more useful to the applied data analyst than the actual variance measure itself. Since the principal components are orthogonal, we can sum the variances to obtain a total measure of variation in the original set of predictors accounted for by the principal components. Below, we compute the proportion of variance in the predictor space that each principal component in our example accounts for:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compute proportion of variation\nvar_pc / sum(var_pc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.98057949 0.01942051\n```\n:::\n:::\n\n\nHere the first principal component accounts for 98.1% of the variation in the predictor space, and the second principal component accounts for the remaining 1.9% of the variation.\n\n\n<br />\n\n\n## Using `princomp()` to Obtain the Principal Components\n\nWe can also use the R function `princomp()` to obtain the principal components based on the spectral decomposition. We provide this function with a data frame of the predictors.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Select predictors\neeo_pred = eeo |>\n  select(faculty, peer)\n\n# Create princomp object\nmy_pca = princomp(eeo_pred)\n\n# View output\nsummary(my_pca, loadings = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImportance of components:\n                          Comp.1     Comp.2\nStandard deviation     1.4002088 0.19725327\nProportion of Variance 0.9805406 0.01945935\nCumulative Proportion  0.9805406 1.00000000\n\nLoadings:\n        Comp.1 Comp.2\nfaculty  0.763  0.647\npeer     0.647 -0.763\n```\n:::\n:::\n\n\nThe values of the principal components, the rotated set of basis vectors, are given in the `loadings` output. Note that the signs of the principal components are arbitrary. For example, the vector associated with the first principal component could also have been $(-0.763, -0.647)$ and that for the second principal component could have been $(-0.647, 0.763)$. The variances of each component can be computed by squaring the appropriate standard deviations in the output.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compute variance of PC1\n1.4002088 ^ 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.960585\n```\n:::\n\n```{.r .cell-code}\n# Compute variance of PC2\n0.19725327 ^ 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03890885\n```\n:::\n:::\n\n\nWe can use the variance measures to obtain a total measure of variation in the original set of predictors accounted for by each of the principal components.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compute total variation accounted for\ntotal_var = 1.4002088 ^ 2 + 0.19725327 ^ 2\ntotal_var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.999494\n```\n:::\n\n```{.r .cell-code}\n# Compute variation accounted for by PC1\n(1.4002088 ^ 2) / total_var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9805406\n```\n:::\n\n```{.r .cell-code}\n# Compute variation accounted for by PC2\n(0.19725327 ^ 2) / total_var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01945935\n```\n:::\n:::\n\n\nThis suggests that the first principal component accounts for 98% of the variance in the original set of predictors and that the second principal component accounts for 2% of the variance. (Same as we computed in the matrix algebra.) Note that these values are also given in the `summary()` output.\n\nWe can also obtain the principal component scores (the values under the rotation) for each observation by accessing the `scores` element of the `princomp` object. (Below we only show the first six scores.)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Get PC scores\npc_scores = my_pca$scores\n\n# View PC scores\nhead(pc_scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Comp.1      Comp.2\n[1,]  0.41884376  0.37000669\n[2,]  0.84765375  0.15132881\n[3,] -1.09849740 -0.05870659\n[4,] -1.81031365  0.12065755\n[5,] -0.05471761  0.25713367\n[6,]  0.16934323  0.03700331\n```\n:::\n:::\n\n\nThese are slightly different than the scores we obtained by multiplying the original predictor values by the new basis matrix. For example, the PC scores for the first observation were $-0.486$ and $0.367$. The `princomp()` function mean centers each variable prior to multiplying by the basis matrix.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Mimic scores from princomp()\nold = t(c(0.608 - mean(eeo$faculty), 0.0351 - mean(eeo$peer)))\n\n# Compute PC scores\nold %*% basis\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]      [,2]\n[1,] 0.4186997 0.3699581\n```\n:::\n:::\n\n\n:::fyi\nBecause we want the principal components to be solely functions of the predictors, we mean center them. Otherwise we would have to include a column of ones (intercept) in the $\\mathbf{X}_p$ matrix. Mean centering the predictors makes each predictor orthogonal to the intercept, in which case it can be ignored. (This is similar to how mean centering predictors removes the intercept from the fitted equation.)\n:::\n\nSince we only have two principal components, we can visualize the scores using a scatterplot.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create data frame of scores\npc_scores = pc_scores |>\n  data.frame()\n\n# Plot the scores\nggplot(data = pc_scores, aes(x = Comp.1, y = Comp.2)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"lightgrey\") +\n  geom_vline(xintercept = 0, color = \"lightgrey\") +\n  scale_x_continuous(name = \"Principal Component 1\", limits = c(-4, 4)) +\n  scale_y_continuous(name = \"Principal Component 2\", limits = c(-4, 4)) +\n  theme_bw() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![Rotated predictor space using the principal components as the new basis.](06-02-pca-via-spectral-decomposition_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nConceptually this visualization shows the rotated predictor space after re-orienting the rotated coordinate system. From this visualization, it is also clear that there is much more variation in the values of the first principal component than in the second principal component.\n\n<br />\n\n\n# Using All Three Predictors in the PCA\n\nIn the example we were focused on only two predictors, as it is pedagogically easier to conceptualize since the plotting is easier. However, PCA is directly extensible to more than two variables. With three variables, the data ellipse is a data ellipsoid and there are three principal components corresponding to the three orthogonal semi-axes of the ellipsoid. With four or more variables the ideas extend although the visual doesn't.\n\nWe will again use the `princomp()` function to compute the principal components and rotated scores.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Select predictors\neeo_pred = eeo |>\n  select(faculty, peer, school)\n\n# Create princomp object\nmy_pca = princomp(eeo_pred)\n\n# View output\nsummary(my_pca, loadings = TRUE, cutoff = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImportance of components:\n                          Comp.1     Comp.2     Comp.3\nStandard deviation     1.7277218 0.19747823 0.09021075\nProportion of Variance 0.9844548 0.01286135 0.00268389\nCumulative Proportion  0.9844548 0.99731611 1.00000000\n\nLoadings:\n        Comp.1 Comp.2 Comp.3\nfaculty  0.617  0.670  0.412\npeer     0.524 -0.741  0.420\nschool   0.587 -0.043 -0.809\n```\n:::\n:::\n\n\n:::fyi\nThe `cutoff=0` argument in the `summary()` function prints all the loadings. By default, only loadings above 0.1 are displayed.\n:::\n\n\n\nThe rotation matrix, or loading matrix, is:\n\n$$\n\\mathbf{P} =\\begin{bmatrix}0.617 & 0.670 & 0.412 \\\\ 0.524 & -0.741 & 0.420 \\\\ 0.587 & -0.043 & -0.809\\end{bmatrix}\n$$\n\nThe first principal component again accounts for most of the variance in the three predictors (98.4%). The other two principal components account for an additional 1.3% and 0.3% of the variance in the predictor space.\n\n<br />\n\n\n# Interpreting the Principal Components\n\nRemember that each principal component is defining a composite variable composed of all three predictors. The loadings are the weights in computing the composite variables. However, they can also be interpreted as the correlations between each particular variable and the composite. And, although the signs are arbitrary, we can try to interpret the differences in direction. So, for example,\n\n- The composite variable formed by the first principal component is moderately and positively correlated with all three predictors.\n- The second composite variable is highly positively correlated with the faculty variable, highly negatively correlated with the peer variable, and not correlated with the school variable.\n- The third composite variable is positively and moderately correlated with the faculty and peer variables, and highly negatively correlated with the school variable.\n\nSometimes these patterns of correlations can point toward an underlying latent factor, but this is a subjective call by the researcher based on their substantive knowledge. Other times the patterns make no sense; the results, after all, are just a mathematical result based on the variances and covariances of the predictors used.\n\nHere, the first composite might be interpreted as an overall measurement of the three predictors since all the loadings are in the same direction and at least of moderate size. The second composite seems to represent a contrast between faculty credentials and peer influence due to the opposite signs on these loadings. While the third composite points toward a more complex contrast between school facilities and the combined faculty credentials/peer group influence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "06-02-pca-via-spectral-decomposition_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}