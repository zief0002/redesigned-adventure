<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-09-29">

<title>EPsy 8264 - üìù Diagnosing Collinearity</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<link href="../site_libs/pagedtable/css/pagedtable.css" rel="stylesheet">
<script src="../site_libs/pagedtable/js/pagedtable.js"></script>
<script src="https://kit.fontawesome.com/e5da75ca36.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../assets/sticky-notes.css">
<link rel="stylesheet" href="../assets/style.css">
<link rel="stylesheet" href="../assets/notes.css">
<link rel="stylesheet" href="../assets/table-styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">EPsy 8264</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../schedule.html" rel="" target="">
 <span class="menu-text">Schedule</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../assignments.html" rel="" target="">
 <span class="menu-text">Assignments</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../data.html" rel="" target="">
 <span class="menu-text">Data</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">Instructor &amp; TA</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../mission-statements.html" rel="" target="">
 <span class="menu-text">Mission Statements</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../umn-policies.html" rel="" target="">
 <span class="menu-text">University Policies</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#regression-analysis" id="toc-regression-analysis" class="nav-link active" data-scroll-target="#regression-analysis">Regression Analysis</a></li>
  <li><a href="#what-is-collinearity" id="toc-what-is-collinearity" class="nav-link" data-scroll-target="#what-is-collinearity">What is Collinearity?</a>
  <ul class="collapse">
  <li><a href="#effects-of-collinearity" id="toc-effects-of-collinearity" class="nav-link" data-scroll-target="#effects-of-collinearity">Effects of Collinearity</a></li>
  <li><a href="#perfect-collinearity-in-practice-model-mis-specification" id="toc-perfect-collinearity-in-practice-model-mis-specification" class="nav-link" data-scroll-target="#perfect-collinearity-in-practice-model-mis-specification">Perfect Collinearity in Practice: Model Mis-specification</a></li>
  <li><a href="#non-exact-collinearity" id="toc-non-exact-collinearity" class="nav-link" data-scroll-target="#non-exact-collinearity">Non-Exact Collinearity</a></li>
  </ul></li>
  <li><a href="#identifying-collinearity" id="toc-identifying-collinearity" class="nav-link" data-scroll-target="#identifying-collinearity">Identifying Collinearity</a>
  <ul class="collapse">
  <li><a href="#collinearity-diagnostics" id="toc-collinearity-diagnostics" class="nav-link" data-scroll-target="#collinearity-diagnostics">Collinearity Diagnostics</a></li>
  <li><a href="#high-correlations-among-predictors" id="toc-high-correlations-among-predictors" class="nav-link" data-scroll-target="#high-correlations-among-predictors">High Correlations among Predictors</a></li>
  <li><a href="#regress-each-predictor-on-the-other-predictors" id="toc-regress-each-predictor-on-the-other-predictors" class="nav-link" data-scroll-target="#regress-each-predictor-on-the-other-predictors">Regress each Predictor on the Other Predictors</a></li>
  <li><a href="#variance-inflation-factor-vif" id="toc-variance-inflation-factor-vif" class="nav-link" data-scroll-target="#variance-inflation-factor-vif">Variance Inflation Factor (VIF)</a></li>
  <li><a href="#eigenvalues-of-the-correlation-matrix" id="toc-eigenvalues-of-the-correlation-matrix" class="nav-link" data-scroll-target="#eigenvalues-of-the-correlation-matrix">Eigenvalues of the Correlation Matrix</a>
  <ul class="collapse">
  <li><a href="#using-r-to-compute-the-eigenvalues-of-the-correlation-matrix" id="toc-using-r-to-compute-the-eigenvalues-of-the-correlation-matrix" class="nav-link" data-scroll-target="#using-r-to-compute-the-eigenvalues-of-the-correlation-matrix">Using R to Compute the Eigenvalues of the Correlation Matrix</a></li>
  </ul></li>
  <li><a href="#condition-indices" id="toc-condition-indices" class="nav-link" data-scroll-target="#condition-indices">Condition Indices</a></li>
  </ul></li>
  <li><a href="#fixing-collinearity-in-practice" id="toc-fixing-collinearity-in-practice" class="nav-link" data-scroll-target="#fixing-collinearity-in-practice">Fixing Collinearity in Practice</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">üìù Diagnosing Collinearity</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 29, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<hr>
<p>In this set of notes, we will give a brief introduction to empirical diagnostics to detect collinearity. We will use the <em>equal-education-opportunity.csv</em> data provided from <span class="citation" data-cites="Chatterjee:2012">Chatterjee &amp; Hadi (<a href="#ref-Chatterjee:2012" role="doc-biblioref">2012</a>)</span> to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.</p>
<ul>
<li><a href="https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/data/equal-education-opportunity.csv">[CSV]</a></li>
<li><a href="../codebooks/equal-education-opportunity">[Codebok]</a></li>
</ul>
<p>A script file for the analyses in these notes is also available:</p>
<ul>
<li><a href="https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/scripts/06-01-diagnosing-collinearity.R">[R Script File]</a></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(corrr)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Read in data</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>eeo <span class="ot">=</span> <span class="fu">read_csv</span>(<span class="st">"https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/data/equal-education-opportunity.csv"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(eeo)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["achievement"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["faculty"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["peer"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["school"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"-0.43148","2":"0.60814","3":"0.03509","4":"0.16607"},{"1":"0.79969","2":"0.79369","3":"0.47924","4":"0.53356"},{"1":"-0.92467","2":"-0.82630","3":"-0.61951","4":"-0.78635"},{"1":"-2.19081","2":"-1.25310","3":"-1.21675","4":"-1.04076"},{"1":"-2.84818","2":"0.17399","3":"-0.18517","4":"0.14229"},{"1":"-0.66233","2":"0.20246","3":"0.12764","4":"0.27311"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Source the residual_plots() function from the script file</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"../scripts/residual_plots.R"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<section id="regression-analysis" class="level1">
<h1>Regression Analysis</h1>
<p>To examine the RQ, the following model was posited:</p>
<p><span class="math display">\[
\mathrm{Achievement}_i = \beta_0 + \beta_1(\mathrm{Faculty}_i) + \beta_2(\mathrm{Peer}_i) + \beta_3(\mathrm{School}_i) + \epsilon_i
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-01-diagnosing-collinearity_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">Residual plots for the model that includes the main effects of faculty credentials, influence of peer groups, and measure of school facilities to predict variation in student achievement.</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the regression model</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Index plots of several regression diagnostics</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">influenceIndexPlot</span>(lm<span class="fl">.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-01-diagnosing-collinearity_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Diagnostic plots for the model that includes the main effects of faculty credentials, influence of peer groups, and measure of school facilities to predict variation in student achievement.</figcaption>
</figure>
</div>
</div>
</div>
<p>School 28 may be problematic, but removing this observation (work not shown) made little improvement in the residual plots. As such, School 28 was retained in the data. As the assumptions seem reasonably met, we next look to the model-level and coefficient-level output:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model-level information</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">glance</span>(lm<span class="fl">.1</span>), <span class="at">width =</span> <span class="cn">Inf</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 √ó 12
  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC
      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1     0.206         0.170  2.07      5.72 0.00153     3  -148.  306.  318.
  deviance df.residual  nobs
     &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;
1     283.          66    70</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Coefficient-level information</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(lm<span class="fl">.1</span>, <span class="at">conf.int =</span> <span class="fl">0.95</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["conf.low"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["conf.high"],"name":[7],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"-0.06995905","3":"0.2506421","4":"-0.2791193","5":"0.7810260","6":"-0.5703821","7":"0.430464"},{"1":"faculty","2":"1.10125968","3":"1.4105616","4":"0.7807243","5":"0.4377560","6":"-1.7150174","7":"3.917537"},{"1":"peer","2":"2.32205659","3":"1.4812872","4":"1.5675937","5":"0.1217584","6":"-0.6354288","7":"5.279542"},{"1":"school","2":"-2.28099511","3":"2.2204481","4":"-1.0272679","5":"0.3080446","6":"-6.7142627","7":"2.152272"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p>Examining this information we find:</p>
<ul>
<li>20% of the variation in student achievement is explained by the model; which is statistically significant <span class="math inline">\(F(3, 66)=5.72\)</span>; <span class="math inline">\(p=0.002\)</span>.</li>
<li>However, none of the individual coefficients are statistically significant!</li>
</ul>
<p>This is a bit of a paradox since we rejected the model-level null hypothesis that <span class="math inline">\(H_0:\beta_{\mathrm{Faculty~Credentials}}=\beta_{\mathrm{Peer~Influence}}=\beta_{\mathrm{School~Facilities}}=0\)</span>, yet the coefficient-level results are consistent with <span class="math inline">\(H_0:\beta_{\mathrm{Faculty~Credentials}}=0\)</span>, <span class="math inline">\(H_0:\beta_{\mathrm{Peer~Influence}}=0\)</span>, and <span class="math inline">\(H_0:\beta_{\mathrm{School~Facilities}}=0\)</span>. These inconsistencies between the model- and coefficient-level results are typical when there is <em>collinearity</em> in the model.</p>
<p><br></p>
</section>
<section id="what-is-collinearity" class="level1 page-columns page-full">
<h1>What is Collinearity?</h1>
<p>Recall from our introduction to matrix algebra that two vectors are collinear if they span the same subspace. In regression, collinearity occurs when any of the columns of the design matrix, <strong>X</strong>, is a perfect linear combination of the other columns:</p>
<p><span class="math display">\[
\mathbf{X_j} = c_0(\mathbf{1}) + c_1\mathbf{X_1} + c_2\mathbf{X_2} + c_3\mathbf{X_3} + \ldots + c_k\mathbf{X_k}
\]</span></p>
<p>and the constants, <span class="math inline">\(c_1, c_2, c_3,\ldots, c_k\)</span> are not all 0. In this situation, <strong>X</strong> is not of full column rank, and the <span class="math inline">\(\mathbf{X}^{\intercal}\mathbf{X}\)</span> matrix is singular.</p>
<aside>
Sometimes ‚Äòcollinearity‚Äô is referred to as ‚Äòmulticollinearity‚Äô. The two terms are synonomous.
</aside>
<p><br></p>
<section id="effects-of-collinearity" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="effects-of-collinearity">Effects of Collinearity</h2>
<p>If the design matrix is not of full rank, and <span class="math inline">\(\mathbf{X}^{\intercal}\mathbf{X}\)</span> is singular, then the OLS normal equations do not have a unique solution. Moreover, the sampling variances for the coefficient are all infinitely large. To understand why this is the case, we can examine one formula for the sampling variance of a slope in a multiple regression:</p>
<p><span class="math display">\[
\mathrm{Var}(B_j) = \frac{1}{1 - R^2_j} \times \frac{\sigma^2_\epsilon}{(n-1)S^2_j}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(R^2_j\)</span> is the squared multiple correlation for the regression of <span class="math inline">\(X_j\)</span> on the the other predictors;</li>
<li><span class="math inline">\(S^2_j\)</span> is the sample variance of predictor <span class="math inline">\(X_j\)</span> defined by <span class="math inline">\(S^2_j = \dfrac{\sum(X_{ij}-\bar{X}_j)^2}{n-1}\)</span>;</li>
<li><span class="math inline">\(\sigma^2_{\epsilon}\)</span> is the variance of the residuals based on regressing <span class="math inline">\(Y\)</span> on all the <span class="math inline">\(X\)</span>‚Äôs</li>
</ul>
<aside>
Recall that the multiple correlation is the correlation between the outcome and the predicted values.
</aside>
<p>The first term in this product is referred to as the <em>variance inflation factor</em> (VIF). When one of the predictors is perfectly collinear with the others, the value of <span class="math inline">\(R^2_j\)</span> is 1 and the VIF is infinity. Thus the sampling variance of <span class="math inline">\(B_j=\infty\)</span>.</p>
<p><br></p>
</section>
<section id="perfect-collinearity-in-practice-model-mis-specification" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="perfect-collinearity-in-practice-model-mis-specification">Perfect Collinearity in Practice: Model Mis-specification</h2>
<p>In practice, it is unlikely that you will have exact (or perfect) collinearity. When it does happen it is often the result of mis-formulating the model (e.g., including dummy variables in the model for all levels of a categorical variable, as well as the intercept). As an example of this, imagine that you were creating the design matrix for a regression model that included occupational status (employed/not employed) to predict some outcome for 5 cases.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create design matrix</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">b_0 =</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">5</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">employed =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">not_employed =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># View design matrix</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["b_0"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["employed"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["not_employed"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"0"},{"1":"1","2":"1","3":"0"},{"1":"1","2":"0","3":"1"},{"1":"1","2":"0","3":"1"},{"1":"1","2":"1","3":"0"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p>The columns in this design matrix are collinear because we can express any one of the columns as a linear combination of the others. For example,</p>
<p><span class="math display">\[
b_0 = 1(\mathrm{employed}) + 1(\mathrm{not~employed})
\]</span></p>
<p>Checking the rank of this matrix, we find that this matrix has a rank of 2. Since there are three columns, <strong>X</strong> is not full column rank; it is rank deficient.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">rankMatrix</span>(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2
attr(,"method")
[1] "tolNorm2"
attr(,"useGrad")
[1] FALSE
attr(,"tol")
[1] 1.110223e-15</code></pre>
</div>
</div>
<p>Including all three coefficients in the model results in overparameterization. The simple solution here is to drop one of the predictors from the model. This is why we only include a single dummy variable in a model that includes an intercept for a dichotomous categorical predictor.</p>
<aside>
Including the intercept is not imperative, although it has a useful interpretation when using dummy coding. One could also include the two dummy-coded predictors and omit the intercept. This gives the means for the two groups, but does not provide a comparison of those means.
</aside>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create vector of outcomes</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">15</span>, <span class="dv">15</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">30</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data frame of Y and X</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>my_data <span class="ot">=</span> <span class="fu">cbind</span>(Y, X)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>my_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["Y"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["b_0"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["employed"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["not_employed"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"15","2":"1","3":"1","4":"0"},{"1":"15","2":"1","3":"1","4":"0"},{"1":"10","2":"1","3":"0","4":"1"},{"1":"15","2":"1","3":"0","4":"1"},{"1":"30","2":"1","3":"1","4":"0"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Coefficients (including all three terms)</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> employed <span class="sc">+</span> not_employed, <span class="at">data =</span> my_data))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> (Intercept)     employed not_employed 
        12.5          7.5           NA </code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Coefficients (omitting intercept)</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> employed <span class="sc">+</span> not_employed, <span class="at">data =</span> my_data))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    employed not_employed 
        20.0         12.5 </code></pre>
</div>
</div>
<p>If you overparameterize a model with <code>lm()</code>, one or more of the coefficients will not be estimated (the last parameters entered in the model).</p>
<aside>
Constraining some parameters is another way to produce a full rank design matrix. For example the ANOVA model has a constraint that the sum of the effect-coded variable is 0. This constraint ensures that the design matrix will be of full rank.
</aside>
<p><br></p>
</section>
<section id="non-exact-collinearity" class="level2">
<h2 class="anchored" data-anchor-id="non-exact-collinearity">Non-Exact Collinearity</h2>
<p>It is more likely, in practice, that you will have less-than-perfect collinearity, and that this will have an adverse effect on the computational estimates of the coefficients‚Äô sampling variances. Again, we look toward how the sampling variances for the coefficent‚Äôs are computed:</p>
<p><span class="math display">\[
\mathrm{Var}(B_j) = \frac{1}{1 - R^2_j} \times \frac{\sigma^2_\epsilon}{(n-1)S^2_j}
\]</span></p>
<p>When the predictors are completely independent, all of the columns of the design matrix will be orthogonal and the correlation between <span class="math inline">\(X_j\)</span> and the other <span class="math inline">\(X\)</span>s will be 0. In this situation, the VIF is 1 and the second term in the product completely defines the sampling variance. This means that the sampling variance is a function of the model‚Äôs residual variance, sample size, and the predictor‚Äôs variance‚Äîthe factors we typically think of affecting the sampling variance of a coefficient.</p>
<p>In cases where the columns in ths design matrix are not perfectly orthogonal, the correlation between <span class="math inline">\(X_j\)</span> and the other <span class="math inline">\(X\)</span>s is larger than 0. (Perfect collinearity results in <span class="math inline">\(R^2_j=1\)</span>.) For these situations, the VIF has a value that is greater than 1. When this happens the VIF acts as a multiplier of the second term, inflating the the sampling variance and reducing the precision of the estimate (i.e., increasing the uncertainty).</p>
<p>How much the uncertainty in the estimate increases is a function of how correlated the predictors are. Here we can look at various multiple correlations (<span class="math inline">\(R_j\)</span>) between <span class="math inline">\(X_j\)</span> and the predicted values from using the other <span class="math inline">\(X\)</span>‚Äôs to predict <span class="math inline">\(X_j\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<caption>Impact of various R<sub>j</sub> values on the VIF and size of the CI for B<sub>j</sub>.</caption>
<thead>
<tr class="header">
<th style="text-align: right;" data-quarto-table-cell-role="th">R<sub>j</sub></th>
<th style="text-align: right;" data-quarto-table-cell-role="th">VIF</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">CI Factor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">1.00</td>
<td style="text-align: right;">1.00</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">1.01</td>
<td style="text-align: right;">1.01</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.2</td>
<td style="text-align: right;">1.04</td>
<td style="text-align: right;">1.02</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">1.10</td>
<td style="text-align: right;">1.05</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">1.19</td>
<td style="text-align: right;">1.09</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">1.33</td>
<td style="text-align: right;">1.15</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.6</td>
<td style="text-align: right;">1.56</td>
<td style="text-align: right;">1.25</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">1.96</td>
<td style="text-align: right;">1.40</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">2.78</td>
<td style="text-align: right;">1.67</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">5.26</td>
<td style="text-align: right;">2.29</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">Inf</td>
<td style="text-align: right;">Inf</td>
</tr>
</tbody>
</table>


</div>
</div>
<p>For example, a multiple correlation of 0.7 results in a VIF of 1.96, which in turn means that the CI (which is based on the square root of the sampling variance) will increase by a factor of 1.4. This inflation increases the uncertainty of the estimate making it harder to make decisions or understand the effect of <span class="math inline">\(B_j\)</span>.</p>
<p>To sum things up, while perfect collinearity is rare in practice, less-than-perfect collinearity is common. In these cases the VIF will be less than 1, but can still have an adverse effect on the sampling variances; sometimes making them quite large.</p>
<p><br></p>
</section>
</section>
<section id="identifying-collinearity" class="level1">
<h1>Identifying Collinearity</h1>
<p>In our case study example, we were alerted to the possible collinearity by finding that the predictors jointly were statistically significant, but that each of the individual predictors were not. Other signs that you may have collinearity problems are:</p>
<ul>
<li>Large changes in the size of the estimated coefficients when variables are added to the model;</li>
<li>Large changes in the size of the estimated coefficients when an observation is added or deleted;</li>
<li>The signs of the estimated coefficients do not conform to their prior substantively hypothesized directions;</li>
<li>Large SEs on variables that are expected to be important predictors.</li>
</ul>
<p><br></p>
<section id="collinearity-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="collinearity-diagnostics">Collinearity Diagnostics</h2>
<p>We can also empirically diagnose problematic collinearity in the data <span class="citation" data-cites="Belsley:1991 Belsley:1980">(<a href="#ref-Belsley:1991" role="doc-biblioref">D. A. Belsley, 1991</a>; <a href="#ref-Belsley:1980" role="doc-biblioref">D. Belsley, Kuh, &amp; Welsch, 1980</a>)</span>. Before we do, however, it is important that the <strong>functional form of the model</strong> has been correctly specified. Since, a model needs to be specified before we can estimate coefficients or their sampling variances, and collinearity produces unstable estimates of these estimates, collinearity should only be investigated <em>after</em> the model has been satisfactorily specified.</p>
<p>Below we will explore some of the diagnostic tools available to an applied researcher.</p>
<p><br></p>
</section>
<section id="high-correlations-among-predictors" class="level2">
<h2 class="anchored" data-anchor-id="high-correlations-among-predictors">High Correlations among Predictors</h2>
<p>Collinearity can sometimes be anticipated by examining the pairwise correlations between the predictors. If the correlation between predictors is large, this might be indicative of collinearity problems.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>eeo <span class="sc">|&gt;</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(faculty, peer, school) <span class="sc">|&gt;</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">correlate</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["faculty"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["peer"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["school"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"faculty","2":"NA","3":"0.9600806","4":"0.9856837"},{"1":"peer","2":"0.9600806","3":"NA","4":"0.9821601"},{"1":"school","2":"0.9856837","3":"0.9821601","4":"NA"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p>In this example, all three of the predictors are highly correlated with one another. This is likely a good indicator that their may be problems in the estimation of coefficients, inflated standard errors, or both; especially given that the correlations are all very high. Unfortunately the source of collinearity may be due to more than just the simple relationships among the predictors. As such, just examining the pairwise correlations is not enough to detect collinearity (although it is a good first step).</p>
<p><br></p>
</section>
<section id="regress-each-predictor-on-the-other-predictors" class="level2">
<h2 class="anchored" data-anchor-id="regress-each-predictor-on-the-other-predictors">Regress each Predictor on the Other Predictors</h2>
<p>Since collinearity is defined as linear dependence within the set of predictors, a better way to diagnose collinearity than just examining the pairwise correlation coefficients is to regress each of the predictors on the remaining predictors and evaluate the <span class="math inline">\(R^2\)</span> value. If all the <span class="math inline">\(R^2\)</span> values are close to zero there is no collinearity problems. If one or more of the <span class="math inline">\(R^2\)</span> values are close to 1, there is a collinearity problem.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use faculty as outcome; obtain R2</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(faculty <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> peer <span class="sc">+</span> school, <span class="at">data =</span> eeo))<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9733906</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use faculty as outcome; obtain R2</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(peer <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> faculty <span class="sc">+</span> school, <span class="at">data =</span> eeo))<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9669002</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use faculty as outcome; obtain R2</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(school <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> faculty <span class="sc">+</span> peer, <span class="at">data =</span> eeo))<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9879743</code></pre>
</div>
</div>
<p>All three <span class="math inline">\(R^2\)</span> values are quite high, which is indicative of collinearity.</p>
<p>One shortcoming with this method of diagnosing collinearity is that when the predictor space is large, you would need to look at the <span class="math inline">\(R^2\)</span> values from several models. And, while this could be automated in an R function, there are other common methods that allow us to diagnose collinearity.</p>
<p>We will examine three additional common methods statisticians use to empirically detect collinearity: (1) computing variance inflation factors for the coefficients; (2) examining the eigenvalues of the correlation matrix; and (3) examining the condition indices of the correlation matrix.</p>
<p><br></p>
</section>
<section id="variance-inflation-factor-vif" class="level2">
<h2 class="anchored" data-anchor-id="variance-inflation-factor-vif">Variance Inflation Factor (VIF)</h2>
<p>Perhaps the most common method applied statisticians use to diagnose collinaerity is to compute and examine variance inflation factors. Recall that the variance inflation factor (VIF) is an indicator of the degree of collinearity, where VIF is:</p>
<p><span class="math display">\[
\mathrm{VIF} = \frac{1}{1 - R^2_j}
\]</span></p>
<p>The VIF impacts the size of the variance estimates for the regression coefficients, and as such, can be used as a diagnostic of collinearity. In practice, since it is more conventional to use the SE to measure uncertainty, it is typical to use the square root of the VIF as a diagnostic of collinearity in practice. The square root of the VIF expresses the proportional change in the CI for the coefficients. We can use the <code>vif()</code> function from the <strong>car</strong> package to compute the variance inflation factors for each coefficient.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># VIF</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(lm<span class="fl">.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> faculty     peer   school 
37.58064 30.21166 83.15544 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Square root of VIF</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">vif</span>(lm<span class="fl">.1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> faculty     peer   school 
6.130305 5.496513 9.118960 </code></pre>
</div>
</div>
<p>The variances (and hence, the standard errors) for all three coefficients are inflated because of collinearity. The SEs for these coefficients are all more than five times as large as they would be if the predictors were independent.</p>
<p>Remember, the VIF can range from 1 (independence among the predictors) to infinity (perfect collinearity). There is not consensus among statisticians about how high the VIF has to be to constitute a problem. Some references cite <span class="math inline">\(\mathrm{VIF}&gt;10\)</span> as problematic (which increases the size of the CI for the coefficient by a factor of over three); while others cite <span class="math inline">\(\mathrm{VIF}&gt;4\)</span> as problematic (which increases the size of the CI for the coefficient by a factor of two). As you consider what VIF value to use as an indicator of problematic inflation, it is more important to consider what introducing that much uncertainty would mean in your substantive problem. For example, would you be comfortable with tripling the uncertainty associated with the coefficient? What about doubling it? Once you make that decision, you can determine your VIF cutoff.</p>
<p>There are several situations in which high VIF values are expected and not problematic:</p>
<ul>
<li><strong>The variables with high VIFs are control variables, and the variables of interest do not have high VIFs.</strong> Since we would not be interested in inference around the control variables, high VIF values on those variables would not</li>
<li><strong>The high VIFs are caused by the inclusion of powers or products of other variables.</strong> The <em>p</em>-value for a product term is not affected by the multicollinearity. Centering predictors prior to creating the powers or the products will reduce the correlations, but the <em>p</em>-value the products will be exactly the same whether or not you center. Moreover the results for the other effects will be the same in either case indicating that multicollinearity has no adverse consequences.</li>
<li><strong>The variables with high VIFs are indicator (dummy) variables that represent a categorical variable with three or more categories.</strong> This is especially true when the reference category used has a small proportion of cases. In this case, <em>p</em>-values for the indicator variables may be high, but the overall test that all indicators have coefficients of zero is unaffected by the high VIFs. And nothing else in the regression is affected. To avoid the high VIF values in this situaton, just choose a reference category with a larger proportion of cases.</li>
</ul>
<p><br></p>
</section>
<section id="eigenvalues-of-the-correlation-matrix" class="level2">
<h2 class="anchored" data-anchor-id="eigenvalues-of-the-correlation-matrix">Eigenvalues of the Correlation Matrix</h2>
<p>A second common method of evaluating collinearity is to compute and evaluate the eigenvalues of the correlation matrix for the predictors. Recall that each square (<span class="math inline">\(k \times k\)</span>) matrix has a set of <em>k</em> scalars, called eigenvalues (denoted <span class="math inline">\(\lambda\)</span>) associated with it. These eigenvalues can be arranged in descending order such that,</p>
<p><span class="math display">\[
\lambda_1 \geq \lambda_2 \geq \lambda_3 \geq \ldots \geq \lambda_k
\]</span></p>
<p>Because any correlation matrix is a square matrix, we can find a corresponding set of eigenvalues for the correlation matrix. If any of these eigenvalues is exactly equal to zero, it indicates a linear dependence among the variables making up the correlation matrix.</p>
<p>As a diagnostic, rather than looking at the size of all the eigenvalues, we compute the sum of the reciprocals of the eigenvalues:</p>
<p><span class="math display">\[
\sum_{i=1}^k \frac{1}{\lambda_i}
\]</span></p>
<ul>
<li>If the predictors are orthogonal to one another (independent) then <span class="math inline">\(\lambda_i = 1\)</span> and the sum of the reciprocal values will be equal to the number of predictors, <span class="math inline">\(\sum_{i=1}^k \frac{1}{\lambda_i} = k\)</span>.</li>
<li>If the predictors are collinear with one another (dependent) then <span class="math inline">\(\lambda_i = 0\)</span> and the sum of the reciprocal values will be equal to infinity, <span class="math inline">\(\sum_{i=1}^k \frac{1}{\lambda_i} = \infty\)</span>.</li>
<li>When there is nonperfect collinearity then <span class="math inline">\(0 &lt; \lambda_i &lt; 1\)</span>, and the sum of the reciprocal values will be greater than the number of predictors, <span class="math inline">\(\sum_{i=1}^k \frac{1}{\lambda_i} &gt; k\)</span>.</li>
</ul>
<div class="math">
<p>In an orthogonal matrix, the eigenvalues are all <span class="math inline">\(\pm1\)</span>, but since the correlation matrix is positive semidefinite, the eigenvalues are all <span class="math inline">\(+1\)</span>.</p>
</div>
<p>Larger sums of the reciprocal values of the eigenvalues is indicative of higher degrees of collinearity. In practice, we might use some cutoff to indicate when the collinearity is problematic. One such cutoff used is, if the sum is greater than five times the number of predictors, it is a sign of collinearity.</p>
<p><span class="math display">\[
\mathrm{IF} \quad \sum_{i=1}^k \frac{1}{\lambda_i} &gt; 5k \quad \mathrm{THEN} \quad \mathrm{collnearity~is~a~problem}
\]</span></p>
<p><br></p>
<div class="fyi">
<p>In practice, perfect collinearity is rare, but near perfect collinearity can exist and is indicated when at least one of the eigenvalues is near zero, and is quite a bit smaller than the others.</p>
</div>
<p><br></p>
<section id="using-r-to-compute-the-eigenvalues-of-the-correlation-matrix" class="level3">
<h3 class="anchored" data-anchor-id="using-r-to-compute-the-eigenvalues-of-the-correlation-matrix">Using R to Compute the Eigenvalues of the Correlation Matrix</h3>
<p>Because collinearity indicates dependence among the predictors, we would want to compute the eigenvalues for the correlation matrix of the predictors (do not include the outcome when computing this matrix). We can then use the <code>eigen()</code> function to compute the eigenvalues of a square matrix.</p>
<p>In previous classes, I have been using the <code>correlate()</code> function from the <code>{corrr}</code> package to produce correlation matrices. This function produces a formatted output that is nice for displaying the correlation matrix, but, because of its formatting, is not truly a matrix object. Instead, we will use the <code>cor()</code> function, which produces a matrix object, to produce the correlation matrix.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlation matrix of predictors</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>r_xx <span class="ot">=</span> <span class="fu">cor</span>(eeo[<span class="fu">c</span>(<span class="st">"faculty"</span>, <span class="st">"peer"</span>, <span class="st">"school"</span>)])</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>r_xx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          faculty      peer    school
faculty 1.0000000 0.9600806 0.9856837
peer    0.9600806 1.0000000 0.9821601
school  0.9856837 0.9821601 1.0000000</code></pre>
</div>
</div>
<p>Once we have the correlation matrix, we can use the <code>eigen()</code> function to compute the eigenvalues (and eigenvectors) of the inputted correlation matrix.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute eigenvalues and eigenvectors</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">eigen</span>(r_xx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>eigen() decomposition
$values
[1] 2.951993158 0.040047507 0.007959335

$vectors
           [,1]        [,2]       [,3]
[1,] -0.5761385  0.67939712 -0.4544052
[2,] -0.5754361 -0.73197527 -0.3648089
[3,] -0.5804634  0.05130072  0.8126687</code></pre>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sum of reciprocal of eigenvalues</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="dv">1</span> <span class="sc">/</span> <span class="fu">eigen</span>(r_xx)<span class="sc">$</span>values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 150.9477</code></pre>
</div>
</div>
<p>We compare the sum of the reciprocal of the eigenvalues to five times the number of predictors; <span class="math inline">\(5 \times 3 =15\)</span>. Since this sum is greater than 15, we would conclude that there is a collinearity problem for this model.</p>
<p><br></p>
</section>
</section>
<section id="condition-indices" class="level2">
<h2 class="anchored" data-anchor-id="condition-indices">Condition Indices</h2>
<p>A condition number for a matrix and computational task quantifies how sensitive the result is to perturbations in the input data and to roundoff errors made during the solution process. If minor changes to the matrix elements result in large differences in the computation (say of the inverse), we say that the matrix is ‚Äúill-conditioned‚Äù. It is important to note that a condition number applies not only to a particular matrix, but also to the particular computation being carried out. That is, a matrix can be ill-conditioned for inversion while the eigenvalue problem is well-conditioned.</p>
<p>A third common diagnostic measure of collinearity is to compute the <em>condition number</em> of the correlation matrix of the model predictors. This tells us whether small changes in the data will lead to large changes in regression coefficient estimates. To compute the condition number, we need to compute the <em>condition index</em> for each of the eigenvalues of the correlation matrix based on the model predictors. Each eigenvalue has an associated condition index, and the <em>j</em>th eigenvalue‚Äôs condition index is denoted <span class="math inline">\(\kappa_j\)</span>, where,</p>
<p><span class="math display">\[
\kappa_j = \sqrt{\frac{\lambda_{\mathrm{Max}}}{\lambda_j}}
\]</span></p>
<p>and <span class="math inline">\(\lambda_{\mathrm{Max}}\)</span> is the largest eigenvalue. The largest eigenvalue will have a condition index of,</p>
<p><span class="math display">\[
\begin{split}
\kappa_{\lambda_\mathrm{Max}} &amp;= \sqrt{\frac{\lambda_\mathrm{Max}}{\lambda_\mathrm{Max}}} \\[1ex]
&amp;= 1
\end{split}
\]</span></p>
<p>The smallest eigenvalue will have a condition index of</p>
<p><span class="math display">\[
\kappa_{\lambda_\mathrm{Min}} = \sqrt{\frac{\lambda_\mathrm{Max}}{\lambda_\mathrm{Min}}}
\]</span></p>
<p>This value will be larger than 1 since <span class="math inline">\(\frac{\lambda_\mathrm{Max}}{\lambda_\mathrm{Min}}\)</span> will be greater than 1. In general, the largest eigenvalue will have a condition index of 1 and the other condition indices for every other eigenvalue will be larger than one.</p>
<p>The condition number of the correlation matrix is equivalent to the condition index for the smallest eigenvalue, that is,</p>
<p><span class="math display">\[
\kappa = \sqrt{\frac{\lambda_\mathrm{Max}}{\lambda_\mathrm{Min}}}
\]</span></p>
<p>If the condition number is small, it indicates that the predictors are not collinear, whereas large condition numbers are evidence supporting collinearity.</p>
<p>From empirical work, a condition number between 10 and 30 indicates the presence of multicollinearity. When the condition number is larger than 30, the multicollinearity is regarded as strong and corrective action will almost surely need to be taken. Below we compute the condition indices and the condition number for our empirical example.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort eigenvalues from largest to smallest</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">=</span> <span class="fu">sort</span>(<span class="fu">eigen</span>(r_xx)<span class="sc">$</span>values, <span class="at">decreasing =</span> <span class="cn">TRUE</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># View eigenvalues</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>lambda</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.951993158 0.040047507 0.007959335</code></pre>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute condition indices</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">max</span>(lambda) <span class="sc">/</span> lambda)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  1.000000  8.585586 19.258359</code></pre>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute condition number directly</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">max</span>(lambda) <span class="sc">/</span> <span class="fu">min</span>(lambda))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 19.25836</code></pre>
</div>
</div>
<p>The condition number of the correlation matrix, <span class="math inline">\(\kappa = 19.26\)</span>, suggests there is collinearity among the predictors.</p>
<p><br></p>
</section>
</section>
<section id="fixing-collinearity-in-practice" class="level1">
<h1>Fixing Collinearity in Practice</h1>
<p>The presence of collinearity in the predictor space leads to several potential problems, namely that the estimates of the regression coefficients may not be stable (small changes in the data may lead to big changes in the estimates), and that the standard errors/sampling variances are inflated.</p>
<p>Although there are several solutions to ‚Äúfix‚Äù collinearity in practice, none are a magic bullet. Here are three potential fixes:</p>
<ul>
<li>Re-specify the model
<ul>
<li>Drop one (or more) of the collinear predictors‚ÄîThis changes what you are controlling for;</li>
<li>Combine collinear predictors;</li>
</ul></li>
<li>Biased estimation
<ul>
<li>Trade small amount of bias for a reduction in coefficient variability;</li>
</ul></li>
<li>Introduce prior information about the coefficients
<ul>
<li>This can be done formally in the analysis (e.g., Bayesian analysis);</li>
<li>It can be used to give a different model specification.</li>
</ul></li>
</ul>
<p>Note that although collinearity is a data problem, the most common fixes in practice are to change the model. In upcoming notes, we will look at methods for combining collinear predictors and performing biased estimation.</p>
<p>For example, we could alleviate the collinearity by dropping any two of the predictors and re-fitting the model with only one predictor. This would fix the problem, but would be unsatisfactory because the resulting model would not allow us to answer the research question.</p>
<p>The highly correlated relationships between the predictors is an inherent characteristic of the data generating process we are studying. This makes it difficult to estimate the individual effects of the predictors. Instead, we could look for underlying causes that would explain the relationships we found among the predictors and perhaps re-formulate the model using these underlying causes.</p>
<p><br></p>
<!-- # References -->
<!-- <br /> -->



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Belsley:1991" class="csl-entry" role="listitem">
Belsley, D. A. (1991). <em>Conditioning diagnostics, collinearity and weak data in regression</em>. New York: John Wiley &amp; Sons.
</div>
<div id="ref-Belsley:1980" class="csl-entry" role="listitem">
Belsley, D., Kuh, E., &amp; Welsch, R. (1980). <em>Regression diagnostics</em>. New York: Wiley.
</div>
<div id="ref-Chatterjee:2012" class="csl-entry" role="listitem">
Chatterjee, S., &amp; Hadi, A. S. (2012). <em>Regression analysis by example</em>. New York: Wiley.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>