---
title: "Assignment 05"
author: "Collinearity and Dimension Reduction"
header-includes:
   - \usepackage{xcolor}
   - \usepackage{mathtools}
   - \usepackage{tikz}
   - \usepackage{tkz-euclide}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
output: 
  pdf_document:
    highlight: tango
urlcolor: "umn2"
---


This assignment is worth 16 points. 

```{r include=FALSE}
# Load packages
library(knitr)
library(kableExtra)
library(tidyverse)
library(tidymodels)
library(broom)
library(corrr)
library(patchwork)

# Set knitr options
opts_chunk$set(
  echo=FALSE,
  prompt=FALSE, 
  comment=NA, 
  message=FALSE, 
  warning=FALSE, 
  tidy=FALSE, 
  fig.width=6, 
  fig.height=6,
  fig.align='center', 
  out.width='60%'
  )

# Import data
iowa = readr::read_csv("~/Dropbox/My Mac (CEHD-m1752583)/Documents/github/epsy-8264/data/iowa-judges.csv")

# Load residual_plots() function
source("../../scripts/residual_plots.R")
```

<!-- Override the max of 10 columns in bmatrix-->
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother



This assignment is worth 22 points. 


## Exploratory Analysis

**1. Compute and report the correlation matrix of the 12 predictors.**

```{r}
tab_01 = iowa %>%
  select(knowledge:equality) %>%
  correlate()

print(tab_01, width = Inf)
```

\vspace{2em}

**2. Based on the correlations, comment on whether there may be any potential collinearity problems. Explain.**

The pairwise correlations are quite high (many above 0.70) indicating that there will likely be collinearity problems.

\vspace{2em}

**3. Compute and report the eigenvalues for the correlation matrix you created in Question 1.**

```{r}
# Create correlation matrix
X = cor(iowa[ , 6:17])

#Compute eigen values
round(eigen(X)$values, 2)
```

\vspace{2em}

**4. Based on the eigenvalues, comment on whether there may be any potential collinearity problems. Explain.**

Yes these values portend collinearity problems. They sum of the reciprocals of the eigenvalues (`r round(sum(1/eigen(X)$values), 2)`) is not 0 and is greater than five times the number of predictors ($5k=60$).


# Initial Model

**5. Report the coefficient-level output, including the estimated coefficients (beta weights), standard errors, *t*-values, and *p*-values.**

```{r}
# Fit model
iowa2 = iowa %>%
  select(retention:equality) %>%
  scale()
  
  
lm.1 = lm(scale(retention) ~ -1 + scale(knowledge) + scale(perception) +
            scale(punctuality) + scale(attention) + scale(management) + 
            scale(demeanor) + scale(clarity) + scale(promptness) + scale(criticism) + 
            scale(decision) + scale(courteous) + scale(equality), 
          data = iowa, weights = respondents)

tidy(lm.1) %>%
  mutate(term = c("Knowledge", "Perception",  "Punctuality", "Attention", "Management",
                  "Demeanor", "Clarity", "Promptness", "Criticism", "Decision",
                  "Courteous", "Equality")
         ) %>%
  kable(
    format = "latex",
    digits = c(0, 2, 3, 2, 3),
    caption = "Coefficients, standard errors, \\textit{t}-values, and \\textit{p}-values for the predictors included in a standardized model to explain variation in judge retention voting.",
    col.names = c("Predictor", "$\\beta$", "SE", "\\textit{t}", "\\textit{p}"),
    booktabs = TRUE,
    escape = FALSE,
    #align = c("l", rep("d", 4))
  ) %>%
  kable_styling(latex_options = "HOLD_position", full_width = TRUE) %>%
  row_spec(row = 0, align = c("l", "c", "c", "c", "c"))
```




<!-- \vspace{2em} -->

**6. Based on the VIF values, comment on whether there may be any potential collinearity problems. Explain.**

```{r warning=FALSE}
# Compute VIF
car::vif(lm.1)
```

Yes these values portend collinearity problems. Most of the VIF values are greater than 10 indicating exacerbated estimates of the uncertainty associated with several of the coefficients.


\newpage

**7. Using the predictor with the largest VIF value, use the VIF value to indicate how the standard error for this predictor will be impacted by the collinearity.**

The largest VIF value is 96.5 and is associated with the scaled attention variable. This suggests that the SE associated with the scaled attention coefficient is 9.8 times larger than it would be if all the variables in the model were independent.




# Principal Components Analysis

**8. Compute the composite score based on the first principal component for the first observation (Judge John J. Bauercamper). Show your work.**

```{r}
# Obtain vector of PC weights
pc1 = svd(X)$v[ , 1]

# Obtained scaled variable values for Obs. 1
judge_1 = scale(iowa[ , 6:17])[1, ]

# Compute composite value
sum(pc1 * judge_1)
```


$$
\begin{split}
\mathrm{PC}_1 &= -0.286(-0.892) -0.303(-1.072) -0.264(0.177) -0.308(-1.205) -0.299(-0.070) -0.290(-0.621) -0.292(-1.232)\\
&~~~~-0.246(-0.437) -0.294(-0.338) -0.304(-0.466) -0.276(-0.511) -0.295(-0.428) \\[0.5em]
&\approx 2.08
\end{split}
$$

*Note: Sign might be negative on the final answer.*

\vspace{2em}


<!-- In this section you are going to carry out the principal components analysis by using singular value decomposition on the correlation matrix of the predictors. -->

<!-- **9. Carry out the singular value decomposition on the correlation matrix of the predictors. Report the D matrix.** -->

<!-- ```{r} -->
<!-- # Singular value decomposition -->
<!-- diag(round(svd(X)$d, 3)) -->
<!-- ``` -->



<!-- \vspace{2em} -->

<!-- **10. Report the standard deviation for the first principal component based on the values from the D matrix. Show your work.** -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \sqrt{\lambda} &= \sqrt{9.935} \\[0.5em] -->
<!-- &\approx 3.15 -->
<!-- \end{split} -->
<!-- $$ -->

<!-- \newpage -->

<!-- **11. Compute and interpret the proportion of variance accounted for by the first principal component. Show your work.** -->

<!-- ```{r} -->
<!-- # Proportion of variance -->
<!-- svd(X)$d[1] / sum(svd(X)$d) -->
<!-- ``` -->
<!-- The first principal component accounts for 82.8\% of the variation in the predictors. -->

<!-- \vspace{2em} -->




<!-- ## Choosing the Number of Principal Components -->

<!-- Read the section on scree plots (Section 4) [in this web article](https://medium.com/@bioturing/how-to-read-pca-biplots-and-scree-plots-186246aae063). -->

**9. Create a scree plot showing the eigenvalues for the 12 principal components from the previous analysis.**

```{r fig.width=10, fig.height=6, out.width='4.5in', fig.cap="Scree plot show the eigenvalues for each of the principal components."}
plot(x = 1:12, y = svd(X)$d, type = "b", xlab = "Principal Component", ylab = "Eigenvalue")
```


\vspace{2em}

**10. Using the "elbow criterion", how many principal components are sufficient to describe the data? Explain by referring to your scree plot.**

It looks like about two principal components are sufficient to describe the data based on where the elbow is located in the scree plot. The eigenvalues are pretty constant after the second principal component.

\newpage

**11. Using the "Kaiser criterion", how many principal components are sufficient to describe the data? Explain.**

```{r}
svd(X)$d > 1
```

Based on the Kaiser criterion, two principal components are sufficient to describe the data, as only two of the 12 components have an eigenvalue above 1.

\vspace{2em}

**12. Using the "80\% proportion of variance criterion", how many principal components are sufficient to describe the data? Explain.**



```{r}
# Compute cumulative proportion of variance
cumsum(svd(X)$d / sum(svd(X)$d))
```

One principal components is sufficient to describe the data if we use the "80\% proportion of variance criterion", since the first PC explains more than 80\% of the variation.



# Revisit the Regression Analysis

<!-- The evidence from the previous section suggests that the first two principal components are sufficient to explain the variation in the predictor space. -->


**13. By examining the pattern of correlations (size and directions) in the first two principal components, identify the construct defined by the composites of these two components. Explain.**

The first principal component seems to be a general measurement of judges' demeanor and professional competence, as the signs of the weights are all in the same direction and the magnitudes of the weights are all of moderate size. The second PC seems to be measuring a contrast between the demeanor variables and the professional conduct variables, as the signs for these weights are in the opposite direction.

\newpage

**14. Fit the regression analysis using the first two principal components as predictors of retention percentage. (Don't forget your weights.) Create and report the plot of the residuals vs. fitted values. What does this suggest about the validity of the linearity assumption?**


```{r fig.cap="Plot of the standardized residuals versus the fitted values for the model that includes the first two principal components as predictors of retention.", fig.pos="H"}
# Create new data set with PC scores, outcome, and weights
iowa_pc = data.frame(prcomp(iowa[ , 6:17])$x) %>%
  mutate(
    retention = scale(iowa$retention),
    respondents = iowa$respondents
    )

# Fit Model 1
lm.1 = lm(retention ~ PC1 + PC2, data = iowa_pc, weights = respondents)

# Fit Model 2
#lm.2 = lm(retention ~ PC1 + I(PC1 ^ 2) + PC2, data = iowa_pc, weights = respondents)

augment(lm.1) %>%
  ggplot(aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Standardized residuals")

#residual_plots(lm.1)
# residual_plots(lm.2)
```

The curvature of the loess smoother, and non-overlap with the $Y=0$ line, indicates that the average residual at each fitted value is not 0. This implies that the linearity assumption may be violated.

\newpage

**15. Again, fit the same regression model using the first two principal components as predictors of retention percentage, but this time also include a quadratic effect of the first principal component. Create and report the plot of the residuals vs. fitted values. What does this suggest about the validity of the linearity assumption?**

```{r fig.cap="Plot of the standardized residuals versus the fitted values for the model that includes the first two principal components as predictors of retention.", fig.pos="H"}
# Fit Model 2
lm.2 = lm(retention ~ PC1 + I(PC1 ^ 2) + PC2, data = iowa_pc, weights = respondents)

augment(lm.2) %>%
  ggplot(aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth() +
  xlab("Fitted values") +
  ylab("Standardized residuals")
```

The loess smoother aligns with the $Y=0$ line, indicateing that the average residual at each fitted value is 0. This implies that the data are consistent with meeting the linearity assumption.

\vspace{2em}

**16. Use the coefficient-level output from the model fitted in Question 15, interpret the effect of the first principal component.**

There is a positive quadratic effect of the first principal component on retention, controlling for differences in the second principal component. The same amount of changes in PC1 values at lower levels of the scale are associated with higher rate of change in retention voting (on average) and less change in retention voting at higher levels of the PC1.

\newpage

# Influential Values

**17. Based on Cook's *D*, identify the name of any judges (and their Cook's *D* value) that are influential observations.**

```{r}
# Cook's D cutoff
cutoff = 4/61

# Get judges with extreme Cook's D values
augment(lm.2) %>%
  mutate(judge = iowa$judge) %>%
  select(judge, .cooksd) %>%
  filter(.cooksd > cutoff)

# augment(lm.2) %>%
#   mutate(
#     judge = iowa$judge,
#     index = 1:nrow(iowa)
#     ) %>%
#   ggplot(aes(x= index, y = .cooksd)) +
#   geom_text(aes(label = judge)) 
```

*Note: If the decision is based on the index plot, Rachael Seymour, Monica Wittig, Deborah Farmer Minot, and William A. Price would likely be chosen.*

\space{2em}

**18. Remove any influential observations identified in Question 17. Re-fit the same model. Based on comparing the model- and coefficient-level output for this model and the model which included all the observations, comment on how these observations were influencing the $R^2$ value, the estimate of the quadratic effect of PC1, and the effect of PC2.**

```{r}
iowa_pc_2 = iowa_pc %>%
  mutate(
    judge = iowa$judge
  ) %>%
  filter(!judge %in% c("Wittig, Monica", "Brandt, Gregory D.", "Price, William A.", 
                  "Seymour, Racheal", "Farmer Minot, Deborah")
         )

lm.3 = lm(retention ~ PC1 + I(PC1 ^ 2) + PC2, data = iowa_pc_2, weights = respondents)

tidy(lm.2)
tidy(lm.3)
```

After the five influential observations were removed,

- The $R^2$ value decreased from 0.920 to 0.870.
-  The quadratic effect of PC1 value became stronger, decreasing from $-0.109$ to $-0.133$.
-  The effect of PC2 value became weaker, decreasing from 0.076 to 0.036.



<!-- ```{r fig.width=6, fig.height=6, out.width='3.5in'} -->
<!-- # Remove PC2 -->
<!-- lm.2.2 = lm(retention ~ PC1 + I(PC1 ^ 2), data = iowa_pc, weights = respondents) -->

<!-- # Coefficient-level output -->
<!-- tidy(lm.2.2) -->

<!-- # Create plot -->
<!-- ggplot(data = iowa_pc, aes(x = PC1, y = retention)) + -->
<!--   geom_point(size = 3, alpha = 0.3) + -->
<!--   stat_function( -->
<!--     fun = function(x) {0.157 + 0.580*x - 0.109 * x^2},  -->
<!--     color = "blue",  -->
<!--     linetype = "solid" -->
<!--   ) + -->
<!--   theme_bw() + -->
<!--   xlab("Principal Component 1") + -->
<!--   ylab("Standardized retention") -->
<!-- ``` -->

<!-- The effect of the first principal component on retention is non-linear. The same amount of changes in PC values at lower levels of the scale are associated with higher rate of change in retention voting (on average) and less change in retention voting at higher levels of the PC.  -->

<!-- \vspace{2em} -->


